---

#- hosts: tag_class_vcs2vcs
- hosts: tag_class_vcs2vcs
  user: ec2-user
  vars_files:
    - config.yml

  tasks:
    - name: set private_key
      set_fact:
        ansible_ssh_private_key_file: ~/.ssh/ffledgling-keys.pem

    - name: gather facts
      action: ec2_facts
      register: ec2facts

    - name: print instance info
      debug: var=ec2facts

    - name: get instance tags
      ec2_tag: 
        # These aws_*_key needed here because of https://github.com/ansible/ansible/issues/9984
        aws_access_key: "{{ lookup('env', 'AWS_ACCESS_KEY_ID') }}"
        aws_secret_key: "{{ lookup('env', 'AWS_SECRET_ACCESS_KEY') }}"
        resource: "{{ ec2facts.ansible_facts.ansible_ec2_instance_id }}"
        region: "{{ ec2facts.ansible_facts.ansible_ec2_placement_region }}"
        state: list
      register: ec2tags

    - name: print tag info
      debug: var=ec2tags

    - name: attempt to mount volumes assuming they exist
      ec2_vol:
        # These aws_*_key needed here because of https://github.com/ansible/ansible/issues/9984
        aws_access_key: "{{ lookup('env', 'AWS_ACCESS_KEY_ID') }}"
        aws_secret_key: "{{ lookup('env', 'AWS_SECRET_ACCESS_KEY') }}"
        state: present
        name: "{{ item.name }}"
        region: "{{ aws_config.region }}"
      with_items: volumes
      when: ec2tags.tags.name == item.name
      register: volmount
      ignore_errors: yes


    - name: print results of volume matching
      debug: var=volmount

    #- name: create and mount missing volumes
    #  debug: var=item
    #  with_together:
    #    - volmatch.results
    #    - volumes
    #  when: (item.0.failed is defined) and (item.0.failed == True) and (item.0.

    - name: create and mount missing volumes
      ec2_vol:
        # These aws_*_key needed here because of https://github.com/ansible/ansible/issues/9984
        aws_access_key: "{{ lookup('env', 'AWS_ACCESS_KEY_ID') }}"
        aws_secret_key: "{{ lookup('env', 'AWS_SECRET_ACCESS_KEY') }}"
        volume_size: "{{ item.item.volume_size }}"  # conservative, increase for production
        volume_type: "{{ item.item.volume_type }}" # SSD, use standard for magnetic
        device_name: /dev/xvdf
        state: present
        region: "{{ aws_config.region }}"
        instance: "{{ ec2facts.ansible_facts.ansible_ec2_instance_id }}"
      with_items: volmount.results
      when: (item.failed is defined) and (item.failed == true)
      register: createdmountedvols

    - name: print everything
      debug: var=createdmountedvols

      # Not needed for now, trying a different approach

      #    - name: get volumes attached
      #      ec2_vol:
      #        # These aws_*_key needed here because of https://github.com/ansible/ansible/issues/9984
      #        aws_access_key: "{{ lookup('env', 'AWS_ACCESS_KEY_ID') }}"
      #        aws_secret_key: "{{ lookup('env', 'AWS_SECRET_ACCESS_KEY') }}"
      #        state: list
      #        #instance: "{{ ec2facts.ansible_facts.ansible_ec2_instance_id }}"
      #        region: "{{ ec2facts.ansible_facts.ansible_ec2_placement_region }}"
      #      register: ec2volinfo
      #
      #    - name: print vol info
      #      debug: var=ec2volinfo
      #
      #
      #    - name: get volumes tags
      #      ec2_tag: 
      #        # These aws_*_key needed here because of https://github.com/ansible/ansible/issues/9984
      #        aws_access_key: "{{ lookup('env', 'AWS_ACCESS_KEY_ID') }}"
      #        aws_secret_key: "{{ lookup('env', 'AWS_SECRET_ACCESS_KEY') }}"
      #        resource: "{{ item.id }}"
      #        # Not use item.zone, because item.zone is unacceptable subset of region to ansible apparently (us-west-2a vs us-west-2)
      #        region: "{{ item.zone }}"
      #        region: "{{ ec2facts.ansible_facts.ansible_ec2_placement_region }}"
      #        state: list
      #      with_items: ec2volinfo.volumes
      #      register: ec2voltags
      #
      #    - name: print vol tag info
      #      debug: var=ec2voltags



