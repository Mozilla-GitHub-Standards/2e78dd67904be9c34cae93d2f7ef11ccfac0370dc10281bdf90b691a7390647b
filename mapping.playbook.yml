# The problem with having a playbook like this is that, if any part fails,
# re-running it will be a problem without actually resetting everything back to
# scratch.
# For now this needs to be done manually, and is a pain.
---

#- hosts: tag_class_vcs2vcs
- hosts: tag_class_vcs2vcs
  user: ec2-user
  vars_files:
    - config.yml

  tasks:
    - name: set private_key
      set_fact:
        ansible_ssh_private_key_file: ~/.ssh/ffledgling-keys.pem

    - name: gather facts
      action: ec2_facts
      register: ec2facts

    - name: print instance info
      debug: var=ec2facts

    - name: get instance tags
      ec2_tag: 
        # These aws_*_key needed here because of https://github.com/ansible/ansible/issues/9984
        aws_access_key: "{{ lookup('env', 'AWS_ACCESS_KEY_ID') }}"
        aws_secret_key: "{{ lookup('env', 'AWS_SECRET_ACCESS_KEY') }}"
        resource: "{{ ec2facts.ansible_facts.ansible_ec2_instance_id }}"
        region: "{{ ec2facts.ansible_facts.ansible_ec2_placement_region }}"
        state: list
      register: ec2tags

    - name: print tag info
      debug: var=ec2tags

    - name: attempt to mount volumes assuming they exist, debug matches only
      debug: var=item
      with_items: volumes
      when: ec2tags.tags.name == item.name
      ignore_errors: yes

    - name: attempt to mount volumes assuming they exist
      ec2_vol:
        # These aws_*_key needed here because of https://github.com/ansible/ansible/issues/9984
        aws_access_key: "{{ lookup('env', 'AWS_ACCESS_KEY_ID') }}"
        aws_secret_key: "{{ lookup('env', 'AWS_SECRET_ACCESS_KEY') }}"
        instance: "{{ ec2facts.ansible_facts.ansible_ec2_instance_id }}"
        state: present
        name: "{{ item.name }}" # We need the 'Name' tag here, need to figure out what to do
        region: "{{ aws_config.region }}"
      with_items: volumes
      when: ec2tags.tags.name == item.name
      register: volmount
      ignore_errors: yes


    - name: print results of volume matching
      debug: var=volmount

    #- name: create and mount missing volumes
    #  debug: var=item
    #  with_together:
    #    - volmatch.results
    #    - volumes
    #  when: (item.0.failed is defined) and (item.0.failed == True) and (item.0.

    - name: create and mount missing volumes
      ec2_vol:
        # These aws_*_key needed here because of https://github.com/ansible/ansible/issues/9984
        aws_access_key: "{{ lookup('env', 'AWS_ACCESS_KEY_ID') }}"
        aws_secret_key: "{{ lookup('env', 'AWS_SECRET_ACCESS_KEY') }}"
        volume_size: "{{ item.item.volume_size }}"  # conservative, increase for production
        volume_type: "{{ item.item.volume_type }}" # SSD, use standard for magnetic
        device_name: /dev/xvdf
        state: present
        region: "{{ aws_config.region }}"
        instance: "{{ ec2facts.ansible_facts.ansible_ec2_instance_id }}"
      with_items: volmount.results
      when: (item.failed is defined) and (item.failed == true)
      register: createdmountedvols

    - name: print mounted vols info
      debug: var=createdmountedvols

    - name: debug items
      debug: var=item
      when: item|success
      with_items: createdmountedvols.results

    - name: tag the volumes after creation
      ec2_tag: 
        # These aws_*_key needed here because of https://github.com/ansible/ansible/issues/9984
        aws_access_key: "{{ lookup('env', 'AWS_ACCESS_KEY_ID') }}"
        aws_secret_key: "{{ lookup('env', 'AWS_SECRET_ACCESS_KEY') }}"
        resource: "{{ item.volume_id }}"
        # Not use item.zone, because item.zone is unacceptable subset of region to ansible apparently (us-west-2a vs us-west-2)
        region: "{{ aws_config.region}}"
        state: present
        tags:
            # Having both 'Name' and 'name' is not a mistake, 'Name' is need for some of the Amazon and Ansible APIs to work properly
            # For example to mount existing volume by name
            Name: "{{ item.item.item.name }}"
            name: "{{ item.item.item.name }}"
            system: "{{ item.item.item.system }}"
            class: "vcs2vcs"
      when: (item|success) and (item.skipped is not defined)
      with_items: createdmountedvols.results
      register: ec2voltags
    
    - name: print vol tag info
      debug: var=ec2voltags


    - name: format freshly created volumes
      # Should format only the ones that are freshly attached, not all of them
      shell: sudo mkfs.ext4 -N $(( $(sudo mkfs.ext4 -n /dev/xvdf | grep -P -o "\d+ inodes" | cut -f 1 -d ' ') * 4 )) "{{ item.device }}"
      when: (item|success) and (item.skipped is not defined)
      with_items: createdmountedvols.results
      register: formattedvols


    - name: mount disks
      sudo: true
      # Should mount/should ensure mounted for all machines
      # For now mounting only /dev/xvdf to /opt/vcs2vcs
      # TODO: Mount disks to mount-pts mentioned in config.yml (missing atm)
      mount:
          state: mounted
          name: /opt/vcs2vcs
          src: /dev/xvdf
          fstype: ext4

    # Works till here
    #
################################################################################   
      
      # Not needed for now, trying a different approach

      #    - name: get volumes attached
      #      ec2_vol:
      #        # These aws_*_key needed here because of https://github.com/ansible/ansible/issues/9984
      #        aws_access_key: "{{ lookup('env', 'AWS_ACCESS_KEY_ID') }}"
      #        aws_secret_key: "{{ lookup('env', 'AWS_SECRET_ACCESS_KEY') }}"
      #        state: list
      #        #instance: "{{ ec2facts.ansible_facts.ansible_ec2_instance_id }}"
      #        region: "{{ ec2facts.ansible_facts.ansible_ec2_placement_region }}"
      #      register: ec2volinfo
      #
      #    - name: print vol info
      #      debug: var=ec2volinfo
      #
      #
      #    - name: get volumes tags
      #      ec2_tag: 
      #        # These aws_*_key needed here because of https://github.com/ansible/ansible/issues/9984
      #        aws_access_key: "{{ lookup('env', 'AWS_ACCESS_KEY_ID') }}"
      #        aws_secret_key: "{{ lookup('env', 'AWS_SECRET_ACCESS_KEY') }}"
      #        resource: "{{ item.id }}"
      #        # Not use item.zone, because item.zone is unacceptable subset of region to ansible apparently (us-west-2a vs us-west-2)
      #        region: "{{ item.zone }}"
      #        region: "{{ ec2facts.ansible_facts.ansible_ec2_placement_region }}"
      #        state: list
      #      with_items: ec2volinfo.volumes
      #      register: ec2voltags
      #
      #    - name: print vol tag info
      #      debug: var=ec2voltags



